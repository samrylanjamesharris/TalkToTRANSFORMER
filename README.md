# Talk To TRANSFORMER
This document explains the eveloution of transformer models and how their impact of AI has chnaged dramatically.

### Transformer:
- First introduced in 2017 as a spread sheet explaining how we can turn a Neural Network into a "self-attention" mechanism.
- This helps take in all the infomation instead of bit by bit.
- It can help understand the context with word-relationships and help with potential image finding or langauge translation.

### Evolution:
#### 1. BERT:
- BERT was Google's first one to come into play as it could be trained on wikipeda infomation.
- This wasn't necessarily text-genration but fouced on lanuage understanding.

#### 2. GPT:
- OpenAI then stepped in and actually fouced on making an text-genration AI.
- It was trained on 7,000 books.
- It wasn't the best and faced an issue with repetition and a lack of fine-tuning.

#### 3. GPT-2:
- GPT-2 then came a year later and gained an upgrade of 1.5B parameters.
- This was able to actually continue off the prompt and could give coherent responses.

#### 4. GPT-3.5:
- This gained a massive upgrade of 175B parameters.
- It could code, do poetry, roleplay, etc.
- This was boosted with ChatGPT's release.

If you want more infomation about how it continued off here, then go to this [website](https://samrylanjamesharris.github.io/TalkToTRANSFORMER/TTT.html) where the document sits.
